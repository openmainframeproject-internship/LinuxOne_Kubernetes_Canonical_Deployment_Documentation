= Installing Kubernetes on LinuxOne - Canonical Distribution
Rajula Vineet Reddy <rajula96reddy@gmail.com>
v1.0, 2018-06-11
:toc:

*Note:* This setup documents running a two node (1 Master & 1 Worker) Kubernetes Cluster
on Canonical Virtual Machines.

## Versions of all the components used
1. LinuxOne Instance running on *SLES12SP3* and *LinuxONE-Medium* flavor
2. Ubuntu Virtual Machines running on *18.04*
3. Go *1.10.2*
4. Kubernetes *1.10.0*
5. Docker *18.06.0*
6. Etcd *3.3.6*
7. Flannel *0.10.0*

## KVM Setup
### Installing & Using KVM on Linux One CC and SLES12SP3
1. Create a Linux One Community Cloud Account
2. Create an instance using a key pair
3. Start the instance
4. SSH to the instance using the command ```ssh -i key.pem linux1@<ip>```
5. Get root privileges using the command ```sudo -i```
6. Install KVM tools and server using the command ```zypper in -t pattern kvm_server kvm_tools```
7. Start libvirtd using the command ```systemctl start libvirtd.service```
8. To verify installation run the command ```virsh list --all```, this should return nothing as we did not create any VM as of now
9. On your host system (your workspace machine you use, to access the cloud server), install the GUI manager **Virtual Machine Manager** or **virt-manager** using the command ```sudo apt-get install virt-manager```
10. Give appropriate permissions using the command ```usermod -G libvirt <username>``` (here username = linux1)

### Using the KVM from a remote machine
1. From your host system, try connecting to the linux1 hypervisor using the command ```virsh -c qemu+ssh://linux1@<ip>/system?keyfile=<Key path> list```. This will return the same as that of step 8 of installation.
2. To simply the above command, go to the path ```~/.config/libvirt/libvrit.conf```, if the path doesn't exist create the path. In that file add the below contents```uri_aliases=[
"l1cc=qemu+ssh://linux1@<ip>/system?keyfile=<Key path>",
]```The alias word 'l1cc' can be changed anything upto the user
3. Now you can run the ```virsh ..``` command as ```virsh -c l`cc list```
4. To get the GUI run ```virt-manager -c l1cc```. Given some time, remote hypervisor will be connected from your host machine
5. Use the GUI, to first create a network, storage and then start a VM

## Setting up the VMs before installing the components
1. Create two virtual machines after setting up KVM as explained above and Install ubuntu on both the nodes
2. Make sure to set the hostname of both the machines to differentiate one machine from the other. Preferably
name the machines as *_k8s-master_* for the Master node and *_k8s-worker_* for the Worker node
// 3. Resource allocation of the nodes < Working >
3. SSH into the nodes and follow the below steps in the nodes specified across each step
4. Note down the IP addresses of both the nodes. In the context of this document, IP addresses of the nodes
are as follows
+
|====
| Node | IP
| Master | [red]#192.168.100.218#
| Worker | [red]#192.168.100.225#
|====
+
5. Create a root password on both the worker nodes, to able to _scp_
+
....
sudo passwd root
....
+
6. Install *Go* on the Master node
+
....
wget [https://dl.google.com/go/go1.10.2.linux-s390x.tar.gz](https://dl.google.com/go/go1.10.2.linux-s390x.tar.gz)
tar -C /usr/local -xzf go1.10.2.linux-s390x.tar.gz
echo 'export PATH=$PATH:/usr/local/go/bin' >> /etc/profile
source /etc/profile
....
+
7. Install vim, git on both the nodes
+
....
sudo apt-get -y install vim git net-tools
....
+
8. Install docker on both the nodes
+
....
cd ~/Downloads
wget [https://download.docker.com/linux/ubuntu/dists/bionic/pool/nightly/s390x/docker-ce_18.06.0~ce~dev~git20180604.170905.0.7841994-0~ubuntu_s390x.deb](https://download.docker.com/linux/ubuntu/dists/bionic/pool/nightly/s390x/docker-ce_18.06.0~ce~dev~git20180604.170905.0.7841994-0~ubuntu_s390x.deb)
sudo apt-get install libltdl7 #docker dependency
sudo dpkg -i docker-ce*
# sudo systemctl enable docker.service
# sudo systemctl start docker.service
....
+
9. Create folders
  - On Master Node
+
....
sudo mkdir -p /srv/kubernetes
sudo mkdir -p /var/lib/{kube-controller-manager,kube-scheduler}
sudo mkdir /var/lib/flanneld/
sudo mkdir -p
....
+
 - On Worker Node
+
....
sudo mkdir -p /srv/kubernetes
sudo mkdir -p /var/lib/{kube-proxy,kubelet}
sudo mkdir /var/lib/flanneld/
....

## Installing Kubernetes binaries on the Nodes
....
# Run the following on the Master node
cd ~/Downloads
wget https://dl.k8s.io/v1.10.0/kubernetes.tar.gz
tar -xvf kubernetes.tar.gz
cd kubernetes/cluster
export KUBERNETES_SERVER_ARCH=s390x
./get-kube-binaries.sh
cd server
tar -xvf kubernetes-server-linux-s390x.tar.gz
sudo cp server/kubernetes/server/bin/{kubectl,kube-apiserver,kube-controller-manager,kube-scheduler} /usr/local/bin
sudo scp server/kubernetes/server/bin/{kubelet,kube-proxy} k8s-worker@192.168.100.225:~/
# Run the next line on the worker
sudo cp ~/{kubelet,kube-proxy} /usr/local/bin
....
## Creating Certificates & Authentication to enable secure communication across all the Kubernetes components
Run all the following steps and there by generate the files in the Master node, and then copy the
specific mentioned certs and config files to the worker nodes.

### Generating Certificates
#### CA - Certificate Authority
....
mkdir -p /srv/kubernetes
cd /srv/kubernetes
openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
....
#### Master Node OpenSSL config
....
cat > openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.2 = 192.168.100.218 # Master IP
EOF
....
#### Kube-apiserver certificates
....
openssl genrsa -out apiserver-key.pem 2048
openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial \
  -out apiserver.pem -days 7200 -extensions v3_req -extfile openssl.cnf
cp apiserver.pem server.crt
cp apiserver-key.pem server.key
....
#### Admin certificates
....
openssl genrsa -out admin-key.pem 2048
openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=admin"
openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 7200
....
#### Kube-proxy certificates
....
openssl genrsa -out kube-proxy-key.pem 2048
openssl req -new -key kube-proxy-key.pem -out kube-proxy.csr -subj "/CN=kube-proxy"
openssl x509 -req -in kube-proxy.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-proxy.pem -days 7200
....
#### Kubelet certificates
....
openssl genrsa -out kubelet-key.pem 2048
openssl req -new -key kubelet-key.pem -out kubelet.csr -subj "/CN=kubelet"
openssl x509 -req -in kubelet.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kubelet.pem -days 7200
....
#### Kube-controller-manager certificates
....
openssl genrsa -out kube-controller-manager-key.pem 2048
openssl req -new -key kube-controller-manager-key.pem -out kube-controller-manager.csr -subj "/CN=kube-controller-manager"
openssl x509 -req -in kube-controller-manager.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-controller-manager.pem -days 7200
....
#### Kube-scheduler certificates
....
openssl genrsa -out kube-scheduler-key.pem 2048
openssl req -new -key kube-scheduler-key.pem -out kube-scheduler.csr -subj "/CN=kube-scheduler"
openssl x509 -req -in kube-scheduler.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-scheduler.pem -days 7200
....
#### Worker OpenSSL config
....
cat > worker-openssl.cnf << EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = 192.168.100.225
EOF
....
#### Worker certificates
....
openssl genrsa -out ubuntu-worker-key.pem 2048
WORKER_IP=192.168.100.225 openssl req -new -key ubuntu-worker-key.pem -out ubuntu-worker.csr \
  -subj "/CN=ubuntu" -config worker-openssl.cnf
WORKER_IP=192.168.100.225 openssl x509 -req -in ubuntu-worker.csr -CA ca.pem -CAkey ca-key.pem \
  -CAcreateserial -out ubuntu-worker.pem -days 7200 -extensions v3_req -extfile worker-openssl.cnf
....
#### Etcd OpenSSL config
....
cat > etcd-openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth,serverAuth
subjectAltName = @alt_names
[alt_names]
IP.1 = 192.168.100.218
EOF
....
#### Etcd certificates
....
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -out etcd.csr -subj "/CN=etcd" -extensions v3_req -config etcd-openssl.cnf -sha256
openssl x509 -req -sha256 -CA ca.pem -CAkey ca-key.pem -CAcreateserial \
  -in etcd.csr -out etcd.crt -extensions v3_req -extfile openssl-etcd.cnf -days 7200
....
#### Copy the required certificates to the Worker node
....
scp ca.pem etcd.crt etcd.key server.crt server.key root@192.168.100.225:/srv/kubernetes/
....
### Generating Kubeconfig files
#### Admin Kubeconfig
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.100.218
kubectl config set-credentials admin --client-certificate=/srv/kubernetes/admin.pem --client-key=/srv/kubernetes/admin-key.pem --embed-certs=true --token=$TOKEN
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=admin
kubectl config use-context linux1.k8s
cat ~/.kube/config #Create config file
....
#### Kube-controller-manager Kubeconfig
....
sudo mkdir -p /var/lib/kube-controller-manager
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.100.218 --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-credentials kube-controller-manager --client-certificate=/srv/kubernetes/kube-controller-manager.pem --client-key=/srv/kubernetes/kube-controller-manager-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-controller-manager --kubeconfig=/var/lib/kube-controller-manager/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
....
#### Kube-scheduler Kubeconfig
....
sudo mkdir -p /var/lib/kube-controller-manager
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.100.218 --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-credentials kube-scheduler --client-certificate=/srv/kubernetes/kube-scheduler.pem --client-key=/srv/kubernetes/kube-scheduler-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-scheduler --kubeconfig=/var/lib/kube-scheduler/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-scheduler/kubeconfig
....
#### Kubelet Kubeconfig (for Worker Node)
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.100.218 --kubeconfig=kubelet.kubeconfig
kubectl config set-credentials kubelet --client-certificate=/srv/kubernetes/kubelet.pem --client-key=/srv/kubernetes/kubelet-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=kubelet.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kubelet --kubeconfig=/var/lib/kubelet/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=kubelet.kubeconfig
scp kube-proxy.kubeconfig root@192.168.100.225:/var/lib/kubelet/kubeconfig
....
#### Kube-proxy Kubeconfig (for Worker Node)
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://192.168.100.218 --kubeconfig=kube-proxy.kubeconfig
kubectl config set-credentials kube-proxy --client-certificate=/srv/kubernetes/kube-proxy.pem --client-key=/srv/kubernetes/kube-proxy-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=kube-proxy.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-proxy --kubeconfig=/var/lib/kube-proxy/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=kube-proxy.kubeconfig
scp kube-proxy.kubeconfig root@192.168.100.225:/var/lib/kube-proxy/kubeconfig
....
## Setting up Etcd
### Building the binaries
....
cd $GO_PATH/src/
mkdir -p github.com/coreos
git clone git://github.com/coreos/etcd
cd etcd
git checkout v3.3.6
./build
./test (optional)
cp /bin/* /usr/local/bin
....
### Running Etcd as a systemd service
....
sudo cat > /etc/systemd/system/etcd.service << EOF
[Unit]
Description=etcd key-value store
Documentation=https://github.com/coreos/etcd

[Service]
Environment="ETCD_UNSUPPORTED_ARCH=s390x"
ExecStart=/usr/local/bin/etcd \
 --name master \
 --cert-file=/srv/kubernetes/etcd.crt \
 --key-file=/srv/kubernetes/etcd.key \
 --peer-cert-file=/srv/kubernetes/etcd.crt \
 --peer-key-file=/srv/kubernetes/etcd.key \
 --trusted-ca-file=/srv/kubernetes/ca.pem \
 --peer-trusted-ca-file=/srv/kubernetes/ca.pem \
 --peer-client-cert-auth \
 --client-cert-auth \
 --initial-advertise-peer-urls https://192.168.100.218:2380 \
 --listen-peer-urls https://192.168.100.218:2380 \
 --listen-client-urls https://192.168.100.218:2379,http://127.0.0.1:2379 \
 --advertise-client-urls https://192.168.100.218:2379 \
 --initial-cluster-token etcd-cluster-0 \
 --initial-cluster master=https://192.168.100.218:2380 \
 --initial-cluster-state new \
 --data-dir=/var/lib/etcd \
 --debug
Restart=always
RestartSec=10s
LimitNOFILE=40000
....
## Setting up Flannel
Flannel should be installed on all the nodes

### Installing Flannel
....
cd ~/Downloads
wget https://github.com/coreos/flannel/releases/download/v0.10.0/flanneld-s390x
chmod +x flanneld-s390x
sudo cp flanneld-s390x /usr/local/bin/flanneld
....
#### Adding an entry to etcd
This should be run only once and only on the Master node
....
etcdctl --cert-file /srv/kubernetes/etcd.crt --key-file /srv/kubernetes/etcd.key --ca-file /srv/kubernetes/ca.pem set /coreos.com/network/config '{ "Network": "100.64.0.0/16", "SubnetLen": 24, "Backend": {"Type": "vxlan"} }'
....
### Running Flanneld as a systemd service
....
sudo cat > /etc/systemd/system/flanneld.service << EOF
[Unit]
Description=Network fabric for containers
Documentation=https://github.com/coreos/flannel
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
Restart=always
RestartSec=5
ExecStart= /usr/local/bin/flanneld \
	-etcd-endpoints=https://192.168.100.218:2379 \
  -iface=enc1 \
  -ip-masq=true \
  -subnet-file=/var/lib/flanneld/subnet.env \
  -etcd-cafile=/srv/kubernetes/ca.pem \
  -etcd-certfile=/srv/kubernetes/etcd.crt \
  -etcd-keyfile=/srv/kubernetes/etcd.key

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable flanneld
sudo systemctl start flanneld
sudo systemctl status flanneld --no-pager
....
#### Changing Docker Settings
add the following lines to the _/lib/systemd/system/docker.service_ ```EnvironmentFile=/var/lib/flanneld/subnet.env```
and change the line ```ExecStart=/usr/bin/dockerd -H fd://``` to ```ExecStart=/usr/bin/dockerd -H fd:// --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} --iptables=false --ip-masq=false --ip-forward=true```.
The file should now some what look like
[subs=+quotes]
....
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# FlannelD subnet setup
[red]#EnvironmentFile=/var/lib/flanneld/subnet.env#
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
[red]#ExecStart=/usr/bin/dockerd -H fd:// --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} --iptables=false --ip-masq=false --ip-forward=true
ExecReload=/bin/kill -s HUP $MAINPID#
LimitNOFILE=1048576
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
....
Then run the following commands
....
sudo systemctl daemon-reload
sudo systemctl stop docker
sudo systemctl start docker
....
## Setting up Kubernetes Components
### Master Components
#### Running Kube-api-server as a systemd service
....
sudo cat > /etc/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target etcd.service flanneld.service

[Service]
EnvironmentFile=-/var/lib/flanneld/subnet.env
#User=kube
ExecStart=/usr/local/bin/kube-apiserver \
 --bind-address=0.0.0.0 \
 --advertise_address=192.168.100.218 \
 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \
 --allow-privileged=true \
 --anonymous-auth=false \
 --apiserver-count=3 \
 --authorization-mode=RBAC,AlwaysAllow \
 --authorization-rbac-super-user=admin \
 --etcd-cafile=/srv/kubernetes/ca.pem \
 --etcd-certfile=/srv/kubernetes/etcd.crt \
 --etcd-keyfile=/srv/kubernetes/etcd.key \
 --etcd-servers=https://192.168.100.218:2379 \
 --enable-swagger-ui=true \
 --event-ttl=1h \
 --insecure-bind-address=0.0.0.0 \
 --kubelet-certificate-authority=/srv/kubernetes/ca.pem \
 --kubelet-client-certificate=/srv/kubernetes/kubelet.pem \
 --kubelet-client-key=/srv/kubernetes/kubelet-key.pem \
 --kubelet-https=true \
 --client-ca-file=/srv/kubernetes/ca.pem \
 --runtime-config=api/all=true,batch/v2alpha1=true,rbac.authorization.k8s.io/v1alpha1=true \
 --secure-port=6443 \
 --service-cluster-ip-range=100.65.0.0/24 \
 --storage-backend=etcd2 \
 --tls-cert-file=/srv/kubernetes/apiserver.pem \
 --tls-private-key-file=/srv/kubernetes/apiserver-key.pem \
 --tls-ca-file=/srv/kubernetes/ca.pem \
 --logtostderr=true \
 --v=6
Restart=on-failure
#Type=notify
#LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable kube-apiserver
sudo systemctl start kube-apiserver
sudo systemctl status kube-apiserver --no-pager #Takes time to start receiving requests
....
#### Running Kube-scheduler as a systemd service
....
sudo cat > /etc/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --leader-elect=true \
  --kubeconfig=/var/lib/kube-scheduler/kubeconfig \
  --master=https://192.168.100.218:6443 \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable kube-scheduler
sudo systemctl start kube-scheduler
sudo systemctl status kube-scheduler --no-pager
....
#### Running Kube-controller-manager as a systemd service
....
sudo cat > /etc/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
	--v=2 \
  --allocate-node-cidrs=true \
	--attach-detach-reconcile-sync-period=1m0s \
	--cluster-cidr=100.64.0.0/16 \
	--cluster-name=k8s.virtual.local \
	--leader-elect=true \
	--root-ca-file=/srv/kubernetes/ca.pem \
	--service-account-private-key-file=/srv/kubernetes/apiserver-key.pem \
	--use-service-account-credentials=true \
	--kubeconfig=/var/lib/kube-controller-manager/kubeconfig \
	--cluster-signing-cert-file=/srv/kubernetes/ca.pem \
	--cluster-signing-key-file=/srv/kubernetes/ca-key.pem \
	--service-cluster-ip-range=100.65.0.0/24 \
	--configure-cloud-routes=false \
	--master=https://192.168.100.218:6443
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable kube-controller-manager
sudo systemctl start kube-controller-manager
sudo systemctl status kube-controller-manager --no-pager
....
### Worker Components
#### Running Kubelet as a systemd service
....
sudo cat > /etc/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \
  --allow-privileged=true \
  --cluster-dns=10.0.64.0.10 \
  --cluster-domain=cluster.local \
  --container-runtime=docker \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --serialize-image-pulls=false \
  --register-node=true \
  --tls-cert-file=/srv/kubernetes/server.crt \
  --tls-private-key-file=/srv/kubernetes/server.key \
  --cert-dir=/var/lib/kubelet \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable kubelet
sudo systemctl start kubelet
sudo systemctl status kubelet --no-pager
....
#### Running Kube-proxy as a systemd service
....
sudo cat > /etc/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --cluster-cidr=10.64.0.0/16 \
  --masquerade-all=true \
  --kubeconfig=/var/lib/kube-proxy/kubeconfig \
  --proxy-mode=iptables \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable kube-proxy
sudo systemctl start kube-proxy
sudo systemctl status kube-proxy --no-pager
....
## References
- https://github.com/linux-on-ibm-z/docs/wiki/Building-etcd
- https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step/
- https://github.com/kelseyhightower/kubernetes-the-hard-way/tree/2983b28f13b294c6422a5600bb6f14142f5e7a26/docs
- https://nixaid.com/deploying-kubernetes-cluster-from-scratch/
- https://kubernetes.io
